arch: efficientdet_d3
model_params:
    pretrained: null
    # want to unfreeze encoder because batch is large enough
    encoder_norm_layer: abn
    # as in effdet. but need to check that i really have 90 classes not 80 
    num_classes: 90
optim : fused_sgd
size: 896
batch_size: 16
phases : [{"ep": [0, 10], "lr": [0.001, 0], "mom": 0.9}]